{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Bị lỗi khi cài đặt bằng lệnh PIP nên mình thêm vào\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "OjH_mEO1gFwb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers torch accelerate"
      ],
      "metadata": {
        "id": "geh2kJNDfo3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd1fd6ef-8d7b-4826-ab64-a88bf0dc7f92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/265.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m256.0/265.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9nLLvPufjBL"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
        "# Load bộ tokenzier\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# Cấu hình cho model\n",
        "config = AutoConfig.from_pretrained(\"google/flan-t5-base\", trust_remote_code=True)#, token=token)\n",
        "config.init_device = \"cuda\"\n",
        "config.temperature = 0.1\n",
        "config.max_length =300\n",
        "config.eos_token_id=tokenizer.eos_token_id\n",
        "config.pad_token_id=tokenizer.pad_token_id\n",
        "config.do_sample = True\n",
        "\n",
        "# Laod model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\",device_map=\"cuda\", config=config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"summary this text: ChatGPT is an artificial intelligence (AI) chatbot that uses natural language processing to create humanlike conversational dialogue. The language model can respond to questions and compose various written content, including articles, social media posts, essays, code and emails. ChatGPT is a form of generative AI -- a tool that lets users enter prompts to receive humanlike images, text or videos that are created by AI.ChatGPT is similar to the automated chat services found on customer service websites, as people can ask it questions or request clarification to ChatGPT's replies. The GPT stands for Generative Pre-trained Transformer which refers to how ChatGPT processes requests and formulates responses. ChatGPT is trained with reinforcement learning through human feedback and reward models that rank the best responses. This feedback helps augment ChatGPT with machine learning to improve future responses.\"\n",
        "\n",
        "# Tokenizer đoạn văn bản trên\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Lấy output từ model\n",
        "outputs = model.generate(input_ids)\n",
        "\n",
        "# In ra output\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIjCbi6Hf4yw",
        "outputId": "5364d787-5bfc-496e-b6b1-10b0ef20cfc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> ChatGPT is a chatbot that uses natural language processing to create humanlike dialogue.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Answer the following question: Can you tell me who is the president of US?\"\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "iC2aP_z9yPj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8e7c2a-fe92-4544-914c-9dd9a3ea608f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> Donald Trump</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"I am hungry, i want to eat some thing hot, such as:\"\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkzQoU_aCioi",
        "outputId": "0a3e7914-652b-42b0-a55c-a18bf31a594a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> hot chocolate</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call huggingface API\n",
        "import requests\n",
        "# Khai báo token\n",
        "API_TOKEN=\"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\"\n",
        "\n",
        "# Khai bao URL\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "\n",
        "# Hamnf gọi API và lấy kết quả\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": \"I am thirsty, i need to drink something hot, such as\",\n",
        "})\n",
        "\n",
        "# In ra output\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb-xKGVuhtka",
        "outputId": "b81c2d3d-46a2-4b02-f5fe-a58326fdc4f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'a hot chocolate'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call HuggingFace API\n",
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-large\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": \"I am hungry, i want to eat some\",\n",
        "})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "TZ-VkOGWiZvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2648b563-fca9-4c5a-d544-6616fc304129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'food'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --upgrade\n",
        "!pip install typing-extensions --upgrade"
      ],
      "metadata": {
        "id": "vOS2bdMwk_OE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44eb514-2658-4727-b1ae-a0fab5c38dff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.1 typing-extensions-4.9.0\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from openai import OpenAI\n",
        "\n",
        "# Goi đến openAI để chúng ta hoàn thiện câu\n",
        "client = OpenAI(\n",
        "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Who is the president of US?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "-dVmXIptiKv2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat_completion\n",
        "chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "Vq0zFm2OkMh0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1683790a-da60-41d4-d635-a65d0034120f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As of October 2021, the president of the United States is Joe Biden.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}